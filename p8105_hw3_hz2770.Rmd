---
title: "p8105_hw3_hz2770"
author: "Hao Zheng"
date: "2021/10/16"
output: github_document
---
```{r}
library(tidyverse)
library(patchwork)
```


## Problem 1

First, load the "instacart" dataset.
```{r}
library(p8105.datasets)
data("instacart")
```


Description of the dataset: The dataset "instacart" contains `nrows = 1,384,617`, `ncols = 15`. There are 4 character variables: eval_set, product_name, aisle and department. All the other variables are integer variables.


1. Now, we want to see how many aisles are there in the dataset "instacart" and which aisle has the most orders.

```{r}
sum_by_aisle = 
  instacart %>% 
  group_by(aisle) %>% 
  summarise(n_obs = n())

sum_by_aisle

sum_by_aisle %>% 
  filter(min_rank(desc(n_obs))< 2)
```

There are 134 aisles in total, and the most items are ordered from the aisle "fresh vegetables", which is 150609 in total.



2. Then we try to show the number of items sold in each aisle with a number over 10000 via a scatter plot.

```{r}
filter_sum_by_aisle = 
  sum_by_aisle %>% 
  filter(n_obs > 10000)

filter_sum_by_aisle

filter_sum_by_aisle %>%
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_point() +
  labs(x = NULL, y = NULL) +
  scale_y_continuous(
    breaks = c(20000, 60000, 100000, 140000, 180000),
    labels = c("20000", "60000", "100000", "140000", "180000")
  ) +
  theme(axis.text.x = element_text(angle=90, hjust=1))
```

Then we got that there are 39 aisles in total have more than 10000 items sold, among which, "fresh fruits" and "fresh vegetables" have the most sold.



3. Create a table to show the top three items in aisle "baking ingredients", "dog food care" and "packaged vegetables fruits".
```{r}
final_rank =
  instacart %>%
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")
    ) %>% 
  group_by(aisle, product_name) %>% 
  summarise(num_ordered = n()) %>% 
  filter(min_rank(desc(num_ordered)) < 4) %>% 
  mutate(
    rank_in_aisle = min_rank(desc(num_ordered))
    ) %>% 
  arrange(aisle, rank_in_aisle) %>% 
  select(aisle, rank_in_aisle, product_name, num_ordered) %>% 
  knitr::kable()

final_rank
```



4. Now, show the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
apple_and_coffee = 
  instacart %>%
  mutate(day = order_dow +1) %>%
  mutate(day_of_week = lubridate::wday(day, label = TRUE, locale = "English_United States")) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, day_of_week) %>%  
  summarise(
    mean_hour_of_day = mean(order_hour_of_day, na.rm = TRUE)
    ) %>% 
  pivot_wider(
    names_from = "day_of_week",
    values_from = "mean_hour_of_day"
    ) %>% 
  knitr::kable()

apple_and_coffee
```



## Problem 2

First, let us load the dataset brfss_smart2010 from the P8105 datasets.
```{r}
data("brfss_smart2010")

head(brfss_smart2010)
```

First, do some data cleaning.
```{r}
brfss_smart2010 =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%
  mutate(
    response = factor(response, level = c("Poor", "Fair", "Good", "Very good", "Excellent")),
    response = as.numeric(response)
    )
  # arrange(response)

head(brfss_smart2010)
```

1. Find the states that were observed at 7 or more locations in 2002.
```{r}
brfss_smart2010 %>%
  filter(year == 2002) %>%
  distinct(locationdesc, locationabbr) %>%
  group_by(locationabbr) %>%
  summarize(location_num = n()) %>% 
  filter(location_num >= 7)
```
There were 6 states observed at 7 or more locations in 2002, including CT, FL, MA, NC, NJ and PA.


Then do the similar process for year 2010.
```{r}
brfss_smart2010 %>%
  filter(year == 2010) %>%
  distinct(locationdesc, locationabbr) %>% 
  group_by(locationabbr) %>%
  summarize(location_num = n()) %>% 
  filter(location_num >= 7)
```
There were 14 states observed at 7 or more locations in 2010, including CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX and WA.


2. Construct the dataset and make the "spagetti" plot for mean data value for the excellent response through years for different state.
```{r}
brfss_df =
  brfss_smart2010 %>%
  filter(response == 5) %>%
  group_by(locationabbr, year) %>%
  mutate(mean_data_value = mean(data_value)) %>%
  select(year, locationabbr, mean_data_value) %>%
  distinct()

brfss_df %>%   
  ggplot(
    aes(x = year, y = mean_data_value)
    ) + 
  geom_line(aes(group = locationabbr, color = locationabbr)) +
  labs(
    title = "Mean data value among different states",
    x = "Year",
    y = "Mean Data Value") 

```


3. Make the two-panel graph to show the required distribution.
```{r}
brfss_2006 = 
  brfss_smart2010 %>% 
  filter(year == 2006 & locationabbr == "NY") %>% 
  ggplot(
    aes(x = response, fill = response)
    ) +
  geom_density(alpha = .4, adjust = .5, color = "blue")

brfss_2010 = 
  brfss_smart2010 %>% 
  filter(year == 2010 & locationabbr == "NY") %>% 
  ggplot(
    aes(x = response,y = data_value, color = response)
    )+
  geom_point()

brfss_2006 + brfss_2010
```
Now we get the plot for the response in NY in year 2006 and 2010. From the plot, we can see that the results are quite similar.

## Problem 3

1. Load and clean the accel_data.
```{r}
getwd()

accel_data = read_csv("./data/accel_data.csv")

head(accel_data)
```

Now, we first clean the data as desired.
```{r}
accel_data =
  accel_data %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  mutate(
    day_type = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "weekday"),
    week = as.integer(week),
    day_id = as.integer(day_id),
    minutes = as.integer(minutes)
    )

accel_data

```
The cleaned accel_data contains 6 variables and 50400 observations. The variables are the week, day_id, day and day type for the day, and the minutes, activity for the minute. All the variables are numeric apart from day and day_type.


2. Aggredate the total activities for each day.
```{r}
accel_data %>% 
  group_by(week, day) %>% 
  summarize(
    sum_activity = sum(activity)
  )
```
The sum of activities for each day seems to increase throughout the week from Monday to Sunday.


3. Make a plot to show the activity counts through the day everyday, and use different colors to indicate what day is it.
```{r}
accel_data %>% 
  ggplot(aes(x = minutes, y = activity, color = day)) +
  geom_point()
```

From this graph, the activity counts increase slightly and then decrease slightly throughout the day. It has four obvious peak at around 400, 650, 1000 and 1250 minutes, and the peak occurs mainly at the end of each week(Thursday, Friday, Saturday and Sunday).